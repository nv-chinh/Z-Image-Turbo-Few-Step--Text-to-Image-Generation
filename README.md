# Z-Image Turbo: Fewâ€‘Step Textâ€‘toâ€‘Image Generation

## Overview

**Z-Image Turbo** is a fast **text-to-image diffusion inference project** built on top of the Z-Image architecture. It focuses on generating high-quality images using **only a few denoising steps** to reduce inference latency, making it suitable for interactive applications and large-scale deployment.

This repository focuses on inference usage only â€” how to run Zâ€‘Image Turbo for textâ€‘toâ€‘image generation, and how its internal pipeline works.

---

## Features

- âš¡ **Fast inference** with very few denoising steps  
- ğŸ§  **Diffusion Transformer (DiT)** backbone  
- ğŸ“ Strong text understanding via **Qwen3-4B text encoder**  
- ğŸ–¼ï¸ High-quality image decoding using **Flux VAE**
- ğŸ” Deterministic and stable sampling, ideal for production use
- ğŸ§© Modular design, easy to integrate with existing diffusion pipelines

---

## Model Pipeline

Below is the endâ€‘toâ€‘end pipeline for textâ€‘toâ€‘image inference using Zâ€‘Image Turbo:
```text
Text Prompt
   â†“
Text Encoder (Qwen3-4B)
   â†“
Text Embeddings
   â†“
Gaussian Noise Latents (z_T)
   â†“
Few-Step Diffusion Transformer (Z-Image Turbo)
   â†“
Denoised Latents (z_0)
   â†“
VAE Decoder (Flux VAE)
   â†“
Final RGB Image

```

### Denoising Process

- The model starts from **Gaussian noise** in latent space  
- At each step, the diffusion model predicts a **noise residual**  
- The scheduler updates the latent state using this prediction  
- Thanks to few-step distillation, **each step performs a large, deterministic denoising jump**

---

## Example Results

This section showcases **example outputs generated by Z-Image Turbo** using a variety of **custom text prompts**. These examples highlight the modelâ€™s ability to preserve **semantic alignment**, **visual quality**, and **stylistic consistency** even under **few-step inference**.

